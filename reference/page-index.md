# PageIndex Investigation Report

## Executive Summary

PageIndexã¯ã€Vectify AIç¤¾ãŒé–‹ç™ºã—ãŸé©æ–°çš„ãª**æ¨è«–ãƒ™ãƒ¼ã‚¹RAGï¼ˆRetrieval-Augmented Generationï¼‰**ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‹RAGã®ã€Œé¡ä¼¼æ€§æ¤œç´¢ã€ã‚’ã€Œæ¨è«–ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã€ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€å°‚é–€æ–‡æ›¸ã®æ¤œç´¢ç²¾åº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã—ãŸï¼ˆFinanceBenchã§98.7%ã®ç²¾åº¦ã‚’é”æˆï¼‰ã€‚

## Core Innovation: ã€Œé¡ä¼¼æ€§ â‰  é–¢é€£æ€§ã€

PageIndexã®æ ¸å¿ƒçš„ãªæ´å¯Ÿã¯ã€**ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼æ€§ã¨çœŸã®é–¢é€£æ€§ã¯ç•°ãªã‚‹**ã¨ã„ã†ã“ã¨ã§ã™ã€‚å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«RAGã¯é¡ä¼¼æ€§ã«ä¾å­˜ã—ã¾ã™ãŒã€PageIndexã¯æ¨è«–ã‚’é€šã˜ã¦é–¢é€£æ€§ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚

## System Architecture

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
    .edgeLabel { color: #A6E22E !important; background-color: transparent !important; }
    .cluster rect { fill: #272822 !important; stroke: #F92672 !important; stroke-width: 2px !important; rx: 5px !important; ry: 5px !important; }
    .node rect, .node circle, .node ellipse, .node polygon, .node path { fill: #272822 !important; stroke: #A6E22E !important; stroke-width: 2px !important; }
    .flowchart-link { stroke: #66D9EF !important; stroke-width: 2px !important; }
  '
}}%%

graph TB
    subgraph Input["ğŸ“„ Document Input"]
        PDF[PDF Document]
        MD[Markdown Document]
    end
    
    subgraph Stage1["ğŸŒ³ Stage 1: Tree Generation"]
        TOC[TOC Detection]
        Extract[Text Extraction]
        Boundary[Section Boundary Detection]
        Tree[Hierarchical Tree Building]
        Summary[Node Summarization]
        
        PDF --> TOC
        MD --> Extract
        TOC --> Extract
        Extract --> Boundary
        Boundary --> Tree
        Tree --> Summary
    end
    
    subgraph Stage2["ğŸ” Stage 2: Reasoning-Based Retrieval"]
        Query[User Query]
        TreeSearch[LLM Tree Search]
        Reasoning[Reasoning Process]
        NodeSelect[Node Selection]
        
        Query --> TreeSearch
        Summary --> TreeSearch
        TreeSearch --> Reasoning
        Reasoning --> NodeSelect
    end
    
    subgraph Output["ğŸ’¡ Answer Generation"]
        Context[Context Extraction]
        Answer[Answer Synthesis]
        
        NodeSelect --> Context
        Context --> Answer
    end

    classDef inputClass fill:#272822,stroke:#FD971F,stroke-width:2px;
    classDef stage1Class fill:#272822,stroke:#A6E22E,stroke-width:2px;
    classDef stage2Class fill:#272822,stroke:#66D9EF,stroke-width:2px;
    classDef outputClass fill:#272822,stroke:#F92672,stroke-width:2px;
    
    class PDF,MD inputClass;
    class TOC,Extract,Boundary,Tree,Summary stage1Class;
    class Query,TreeSearch,Reasoning,NodeSelect stage2Class;
    class Context,Answer outputClass;
```

## Detailed Algorithm Analysis

### 1. Tree Generation Algorithm

#### 1.1 PDF Processing Pipeline

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
    .edgeLabel { color: #A6E22E !important; background-color: transparent !important; }
  '
}}%%

flowchart LR
    subgraph PDFProcessing["PDF Processing"]
        PDF[PDF Input]
        TOCCheck{TOC Exists?}
        ParseTOC[Parse TOC Structure]
        ExtractPages[Extract All Pages]
        
        PDF --> TOCCheck
        TOCCheck -->|Yes| ParseTOC
        TOCCheck -->|No| ExtractPages
        ParseTOC --> ExtractPages
    end
    
    subgraph SectionDetection["Section Detection"]
        TitleDetect[Title Detection via GPT-4]
        BoundaryFind[Find Section Boundaries]
        Validate[Validate with Page Text]
        
        ExtractPages --> TitleDetect
        TitleDetect --> BoundaryFind
        BoundaryFind --> Validate
    end
    
    subgraph TreeBuilding["Tree Building"]
        Hierarchy[Build Hierarchy]
        NodeID[Assign Node IDs]
        PageIndex[Map Page Indices]
        
        Validate --> Hierarchy
        Hierarchy --> NodeID
        NodeID --> PageIndex
    end

    classDef processClass fill:#272822,stroke:#A6E22E,stroke-width:2px;
    classDef detectClass fill:#272822,stroke:#66D9EF,stroke-width:2px;
    classDef buildClass fill:#272822,stroke:#F92672,stroke-width:2px;
```

#### 1.2 Key Functions Implementation

```python
# Core title detection algorithm (simplified)
async def check_title_appearance(item, page_list, model="gpt-4"):
    """
    GPT-4ã‚’ä½¿ç”¨ã—ã¦ãƒšãƒ¼ã‚¸å†…ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¤œå‡º
    ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã§ç©ºç™½ã®ä¸ä¸€è‡´ã‚’ç„¡è¦–
    """
    prompt = f"""
    Check if section "{title}" appears in page_text.
    Do fuzzy matching, ignore space inconsistency.
    
    Reply format:
    {{
        "thinking": <reasoning>,
        "answer": "yes/no"
    }}
    """
    response = await ChatGPT_API(prompt)
    return extract_json(response)

# Hierarchical tree construction
def build_tree_structure(sections, page_boundaries):
    """
    éšå±¤çš„ãƒ„ãƒªãƒ¼æ§‹é€ ã®æ§‹ç¯‰
    - å„ãƒãƒ¼ãƒ‰ã«ã¯ä¸€æ„ã®IDã‚’å‰²ã‚Šå½“ã¦
    - ãƒšãƒ¼ã‚¸å¢ƒç•Œã‚’ä¿æŒ
    - è¦ªå­é–¢ä¿‚ã‚’ç¶­æŒ
    """
    tree = []
    for section in sections:
        node = {
            "title": section.title,
            "node_id": generate_node_id(),
            "start_index": section.start_page,
            "end_index": section.end_page,
            "summary": generate_summary(section.content),
            "nodes": []  # Child nodes
        }
        tree.append(node)
    return tree
```

### 2. Tree Search Algorithm

#### 2.1 LLM-Based Tree Search

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
    .edgeLabel { color: #A6E22E !important; }
  '
}}%%

graph TD
    Query[User Query]
    TreeStruct[Tree Structure]
    
    subgraph SearchProcess["LLM Tree Search Process"]
        Analyze[Analyze Query Intent]
        Traverse[Traverse Tree Nodes]
        Evaluate[Evaluate Node Relevance]
        Reason[Generate Reasoning Chain]
        Select[Select Relevant Nodes]
    end
    
    Query --> Analyze
    TreeStruct --> Traverse
    Analyze --> Traverse
    Traverse --> Evaluate
    Evaluate --> Reason
    Reason --> Select
    
    Select --> NodeList[Selected Node IDs]
    
    classDef inputClass fill:#272822,stroke:#FD971F,stroke-width:2px;
    classDef processClass fill:#272822,stroke:#66D9EF,stroke-width:2px;
    classDef outputClass fill:#272822,stroke:#F92672,stroke-width:2px;
    
    class Query,TreeStruct inputClass;
    class Analyze,Traverse,Evaluate,Reason,Select processClass;
    class NodeList outputClass;
```

#### 2.2 Tree Search Implementation

```python
# Basic LLM tree search
async def tree_search(query, tree_structure, model="gpt-4"):
    """
    æ¨è«–ãƒ™ãƒ¼ã‚¹ã®ãƒ„ãƒªãƒ¼æ¢ç´¢
    LLMãŒæ–‡æ›¸æ§‹é€ ã‚’ç†è§£ã—ã€é–¢é€£ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
    """
    prompt = f"""
    Given query and document tree structure,
    find nodes likely to contain the answer.
    
    Query: {query}
    Tree: {json.dumps(tree_structure)}
    
    Reply:
    {{
        "thinking": <reasoning process>,
        "node_list": [node_ids]
    }}
    """
    
    result = await LLM_API(prompt)
    return json.loads(result)

# Advanced: Monte Carlo Tree Search (commercial version)
class MCTSTreeSearch:
    """
    å•†ç”¨ç‰ˆã§ä½¿ç”¨ã•ã‚Œã‚‹MCTSå®Ÿè£…ï¼ˆæ¦‚å¿µï¼‰
    AlphaGoã«ç€æƒ³ã‚’å¾—ãŸæ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """
    def search(self, query, tree):
        # Selection: UCB1ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒãƒ¼ãƒ‰é¸æŠ
        # Expansion: æœ‰æœ›ãªãƒãƒ¼ãƒ‰ã‚’å±•é–‹
        # Simulation: ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã§ä¾¡å€¤ã‚’è©•ä¾¡
        # Backpropagation: çµæœã‚’ä¼æ’­
        pass
```

### 3. Data Structures

#### 3.1 Tree Node Structure

```json
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve monitors financial system vulnerabilities...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring framework...",
      "nodes": []
    }
  ]
}
```

#### 3.2 Configuration Parameters

```yaml
# pageindex/config.yaml
model: gpt-4o-2024-11-20
toc_check_pages: 20          # TOCæ¤œç´¢ãƒšãƒ¼ã‚¸æ•°
max_pages_per_node: 10       # ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šæœ€å¤§ãƒšãƒ¼ã‚¸æ•°
max_tokens_per_node: 20000   # ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
if_add_node_id: yes          # ãƒãƒ¼ãƒ‰IDè¿½åŠ ãƒ•ãƒ©ã‚°
if_add_node_summary: yes     # ã‚µãƒãƒªãƒ¼è¿½åŠ ãƒ•ãƒ©ã‚°
```

## Performance Characteristics

### Computational Complexity

| Operation | Complexity | Notes |
|-----------|------------|-------|
| Tree Generation | O(n Ã— m) | n=pages, m=sections |
| LLM Tree Search | O(log d Ã— h) | d=degree, h=height |
| MCTS (commercial) | O(b^d) | b=branching factor |
| Context Extraction | O(k) | k=selected nodes |

### Resource Requirements

```mermaid
%%{init: {
  'theme': 'dark'
}}%%

pie title "Resource Usage Distribution"
    "LLM API Calls" : 45
    "PDF Processing" : 20
    "Tree Construction" : 15
    "Memory (Tree Storage)" : 10
    "Context Extraction" : 10
```

## Comparison with Vector RAG

### Traditional Vector RAG vs PageIndex

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
  '
}}%%

graph LR
    subgraph VectorRAG["Traditional Vector RAG"]
        VDoc[Document]
        VChunk[Chunking]
        VEmbed[Embedding]
        VStore[Vector DB]
        VSim[Similarity Search]
        VTop[Top-K Results]
        
        VDoc --> VChunk
        VChunk --> VEmbed
        VEmbed --> VStore
        VStore --> VSim
        VSim --> VTop
    end
    
    subgraph PageIndex["PageIndex RAG"]
        PDoc[Document]
        PTree[Tree Structure]
        PReason[Reasoning]
        PSelect[Node Selection]
        PContext[Relevant Context]
        
        PDoc --> PTree
        PTree --> PReason
        PReason --> PSelect
        PSelect --> PContext
    end
    
    classDef vectorClass fill:#272822,stroke:#F92672,stroke-width:2px,stroke-dasharray: 5 5;
    classDef pageClass fill:#272822,stroke:#A6E22E,stroke-width:2px;
    
    class VDoc,VChunk,VEmbed,VStore,VSim,VTop vectorClass;
    class PDoc,PTree,PReason,PSelect,PContext pageClass;
```

### Key Differences

| Aspect | Vector RAG | PageIndex |
|--------|-----------|-----------|
| **Retrieval Method** | Similarity matching | Reasoning-based search |
| **Document Structure** | Lost in chunking | Preserved in tree |
| **Transparency** | Black box embeddings | Explainable reasoning |
| **Expert Knowledge** | Requires fine-tuning | Direct prompt integration |
| **Infrastructure** | Vector DB required | No DB needed |
| **Accuracy (FinanceBench)** | ~70-80% | 98.7% |

## Technical Implementation Details

### Dependencies

```python
# requirements.txt
openai==1.101.0        # GPT-4 API
pymupdf==1.26.4        # PDF processing
PyPDF2==3.0.1          # PDF backup parser
python-dotenv==1.1.0   # Environment management
tiktoken==0.11.0       # Token counting
pyyaml==6.0.2          # Configuration
```

### API Integration Pattern

```python
# Simplified usage pattern
from pageindex import PageIndexClient

# Initialize client
client = PageIndexClient(api_key="YOUR_KEY")

# Generate tree structure
doc_id = client.submit_document("document.pdf")
tree = client.get_tree(doc_id, node_summary=True)

# Perform reasoning-based retrieval
async def retrieve(query, tree):
    # LLM tree search
    node_ids = await tree_search(query, tree)
    
    # Extract context
    context = extract_node_text(node_ids, tree)
    
    # Generate answer
    answer = await generate_answer(query, context)
    return answer
```

## Use Case Analysis

### Optimal Use Cases

1. **Financial Documents**
   - SEC filings (10-K, 10-Q)
   - Annual reports
   - Earnings transcripts
   - Performance: 98.7% accuracy on FinanceBench

2. **Legal Documents**
   - Contracts
   - Regulatory filings
   - Legal briefs
   - Benefits: Preserves legal structure and cross-references

3. **Technical Manuals**
   - API documentation
   - User guides
   - Technical specifications
   - Benefits: Maintains hierarchical organization

4. **Academic Papers**
   - Research papers
   - Textbooks
   - Dissertations
   - Benefits: Preserves logical flow and citations

### Limitations

1. **Unstructured Documents**
   - Social media posts
   - Chat logs
   - Email threads
   - Limitation: No clear hierarchy to leverage

2. **Real-time Requirements**
   - Sub-second response needed
   - High-frequency queries
   - Limitation: LLM latency overhead

3. **Cost Considerations**
   - High-volume queries
   - Budget constraints
   - Limitation: LLM API costs

## Future Developments

### Announced Features

1. **Enhanced MCTS Implementation**
   - Full Monte Carlo Tree Search
   - Value function learning
   - Improved exploration/exploitation balance

2. **Multi-Document Search**
   - Cross-document reasoning
   - Document collection management
   - Inter-document relationship mapping

3. **PageIndex OCR**
   - Long-context OCR model
   - Preserves document hierarchy
   - Superior to existing OCR tools

4. **Performance Optimizations**
   - Caching strategies
   - Batch processing
   - Reduced API calls

## Conclusions

PageIndexã¯ã€RAGæŠ€è¡“ã«ãŠã‘ã‚‹é‡è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã‚’å®Ÿç¾ã—ã¾ã—ãŸï¼š

1. **æ ¸å¿ƒçš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³**: ã€Œé¡ä¼¼æ€§â‰ é–¢é€£æ€§ã€ã®åŸå‰‡ã«åŸºã¥ãã€æ¨è«–ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’å®Ÿç¾
2. **æŠ€è¡“çš„å„ªä½æ€§**: æ–‡æ›¸æ§‹é€ ã‚’ä¿æŒã—ã€èª¬æ˜å¯èƒ½ãªæ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹ã‚’æä¾›
3. **å®Ÿè¨¼ã•ã‚ŒãŸæ€§èƒ½**: FinanceBenchã§98.7%ã®ç²¾åº¦ã‚’é”æˆ
4. **å®Ÿç”¨çš„ä¾¡å€¤**: å°‚é–€æ–‡æ›¸ã®å‡¦ç†ã«ãŠã„ã¦å¾“æ¥æ‰‹æ³•ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹

ç‰¹ã«ã€é‡‘èã€æ³•å¾‹ã€æŠ€è¡“æ–‡æ›¸ãªã©ã€æ§‹é€ ã¨æ–‡è„ˆãŒé‡è¦ãªå°‚é–€æ–‡æ›¸ã«ãŠã„ã¦ã€PageIndexã¯é©å‘½çš„ãªæ”¹å–„ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚äººé–“ã®å°‚é–€å®¶ãŒæ–‡æ›¸ã‚’èª­ã¿è§£ãæ–¹æ³•ã‚’æ¨¡å€£ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè‡ªç„¶ã§åŠ¹æœçš„ãªæƒ…å ±æ¤œç´¢ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

## Markdown Document Processing: Detailed Implementation

### Overview of Markdown Processing

Markdownæ–‡æ›¸ã®å‡¦ç†ã¯ã€HTMLã‚¿ã‚°ã§ã¯ãªããƒ˜ãƒƒãƒ€ãƒ¼ãƒ¬ãƒ™ãƒ«ï¼ˆ`#`ã®æ•°ï¼‰ã‚’åˆ©ç”¨ã—ã¦éšå±¤æ§‹é€ ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šå˜ç´”ã§é«˜é€Ÿãªå‡¦ç†ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### 1. Tree Generation for Markdown Documents

#### 1.1 Processing Pipeline Architecture

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
    .edgeLabel { color: #A6E22E !important; }
  '
}}%%

flowchart TD
    subgraph Input["ğŸ“ Markdown Input"]
        MD[Markdown File]
        Read[Read Content]
        MD --> Read
    end
    
    subgraph HeaderExtraction["ğŸ” Header Extraction"]
        Parse[Parse Headers]
        CodeBlock[Skip Code Blocks]
        Level[Detect Header Levels]
        
        Read --> Parse
        Parse --> CodeBlock
        CodeBlock --> Level
    end
    
    subgraph TextExtraction["ğŸ“„ Text Content Extraction"]
        Boundaries[Find Section Boundaries]
        Extract[Extract Section Text]
        Token[Count Tokens]
        
        Level --> Boundaries
        Boundaries --> Extract
        Extract --> Token
    end
    
    subgraph TreeThinning["âœ‚ï¸ Optional Tree Thinning"]
        Check{Token Count < Threshold?}
        Merge[Merge Small Nodes]
        Keep[Keep Node]
        
        Token --> Check
        Check -->|Yes| Merge
        Check -->|No| Keep
    end
    
    subgraph TreeBuilding["ğŸŒ³ Tree Construction"]
        Stack[Stack-based Building]
        Hierarchy[Create Hierarchy]
        NodeID[Assign Node IDs]
        
        Merge --> Stack
        Keep --> Stack
        Stack --> Hierarchy
        Hierarchy --> NodeID
    end
    
    subgraph Summarization["ğŸ“ Summarization"]
        Summary{Need Summary?}
        Generate[GPT-4 Summary]
        Prefix[Prefix Summary]
        
        NodeID --> Summary
        Summary -->|Yes| Generate
        Generate --> Prefix
    end
    
    Output[Tree Structure JSON]
    Summary --> Output
    Prefix --> Output
    
    classDef inputClass fill:#272822,stroke:#FD971F,stroke-width:2px;
    classDef extractClass fill:#272822,stroke:#66D9EF,stroke-width:2px;
    classDef buildClass fill:#272822,stroke:#A6E22E,stroke-width:2px;
    classDef outputClass fill:#272822,stroke:#F92672,stroke-width:2px;
```

#### 1.2 Core Algorithm Implementation (TypeScript Pseudocode)

```typescript
// TypeScriptå®Ÿè£…ã®æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰

interface MarkdownNode {
    title: string;
    level: number;  // 1-6 for # to ######
    lineNum: number;
    text: string;
    tokenCount?: number;
}

interface TreeNode {
    title: string;
    nodeId: string;
    text: string;
    lineNum: number;
    summary?: string;
    prefixSummary?: string;
    nodes: TreeNode[];
}

class MarkdownTreeGenerator {
    private headerPattern = /^(#{1,6})\s+(.+)$/;
    private codeBlockPattern = /^```/;
    
    /**
     * Step 1: ãƒ˜ãƒƒãƒ€ãƒ¼æŠ½å‡º
     * ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç„¡è¦–ã—ãªãŒã‚‰ã€Markdownã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æŠ½å‡º
     */
    extractHeaders(markdownContent: string): MarkdownNode[] {
        const lines = markdownContent.split('\n');
        const nodes: MarkdownNode[] = [];
        let inCodeBlock = false;
        
        lines.forEach((line, index) => {
            const trimmedLine = line.trim();
            
            // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹/çµ‚äº†ã‚’æ¤œå‡º
            if (this.codeBlockPattern.test(trimmedLine)) {
                inCodeBlock = !inCodeBlock;
                return;
            }
            
            // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¤–ã§ã®ã¿ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œå‡º
            if (!inCodeBlock) {
                const match = this.headerPattern.exec(trimmedLine);
                if (match) {
                    nodes.push({
                        title: match[2].trim(),
                        level: match[1].length,
                        lineNum: index + 1,
                        text: ''
                    });
                }
            }
        });
        
        return nodes;
    }
    
    /**
     * Step 2: ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡º
     * å„ãƒ˜ãƒƒãƒ€ãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
     */
    extractSectionText(nodes: MarkdownNode[], lines: string[]): MarkdownNode[] {
        return nodes.map((node, index) => {
            const startLine = node.lineNum - 1;
            const endLine = index + 1 < nodes.length 
                ? nodes[index + 1].lineNum - 1 
                : lines.length;
            
            node.text = lines.slice(startLine, endLine).join('\n').trim();
            return node;
        });
    }
    
    /**
     * Step 3: ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã¨éšå±¤çš„é›†è¨ˆ
     * å­ãƒãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ãŸç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
     */
    calculateHierarchicalTokens(nodes: MarkdownNode[]): MarkdownNode[] {
        // å¾Œã‚ã‹ã‚‰å‡¦ç†ã—ã¦ã€å­ãƒãƒ¼ãƒ‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦ªã«é›†è¨ˆ
        for (let i = nodes.length - 1; i >= 0; i--) {
            const currentNode = nodes[i];
            const currentLevel = currentNode.level;
            
            // å­ãƒãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã‚‹
            const childrenIndices = this.findChildren(i, currentLevel, nodes);
            
            // ç·ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨ˆç®—
            let totalText = currentNode.text;
            childrenIndices.forEach(childIndex => {
                totalText += '\n' + nodes[childIndex].text;
            });
            
            currentNode.tokenCount = this.countTokens(totalText);
        }
        
        return nodes;
    }
    
    /**
     * Step 4: ãƒ„ãƒªãƒ¼é–“å¼•ãï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
     * å°ã•ã™ãã‚‹ãƒãƒ¼ãƒ‰ã‚’è¦ªã«ãƒãƒ¼ã‚¸
     */
    performTreeThinning(
        nodes: MarkdownNode[], 
        minTokenThreshold: number
    ): MarkdownNode[] {
        const nodesToRemove = new Set<number>();
        
        for (let i = nodes.length - 1; i >= 0; i--) {
            if (nodesToRemove.has(i)) continue;
            
            const node = nodes[i];
            if (node.tokenCount! < minTokenThreshold) {
                const childrenIndices = this.findChildren(i, node.level, nodes);
                
                // å­ãƒãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ªã«ãƒãƒ¼ã‚¸
                const mergedText = [node.text];
                childrenIndices.forEach(childIndex => {
                    if (!nodesToRemove.has(childIndex)) {
                        mergedText.push(nodes[childIndex].text);
                        nodesToRemove.add(childIndex);
                    }
                });
                
                node.text = mergedText.join('\n\n');
                node.tokenCount = this.countTokens(node.text);
            }
        }
        
        // ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã‚’å‰Šé™¤
        return nodes.filter((_, index) => !nodesToRemove.has(index));
    }
    
    /**
     * Step 5: éšå±¤çš„ãƒ„ãƒªãƒ¼æ§‹é€ ã®æ§‹ç¯‰
     * ã‚¹ã‚¿ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éšå±¤æ§‹é€ ã‚’æ§‹ç¯‰
     */
    buildHierarchicalTree(nodes: MarkdownNode[]): TreeNode[] {
        const stack: Array<[TreeNode, number]> = [];
        const rootNodes: TreeNode[] = [];
        let nodeCounter = 1;
        
        for (const node of nodes) {
            const treeNode: TreeNode = {
                title: node.title,
                nodeId: String(nodeCounter).padStart(4, '0'),
                text: node.text,
                lineNum: node.lineNum,
                nodes: []
            };
            nodeCounter++;
            
            // ã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒãƒ¼ãƒ‰ã‚’å‰Šé™¤
            while (stack.length > 0 && stack[stack.length - 1][1] >= node.level) {
                stack.pop();
            }
            
            // è¦ªãƒãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã¦è¿½åŠ 
            if (stack.length === 0) {
                rootNodes.push(treeNode);
            } else {
                const [parentNode] = stack[stack.length - 1];
                parentNode.nodes.push(treeNode);
            }
            
            stack.push([treeNode, node.level]);
        }
        
        return rootNodes;
    }
    
    /**
     * Helper: å­ãƒãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
     */
    private findChildren(
        parentIndex: number, 
        parentLevel: number, 
        nodes: MarkdownNode[]
    ): number[] {
        const childrenIndices: number[] = [];
        
        for (let i = parentIndex + 1; i < nodes.length; i++) {
            if (nodes[i].level <= parentLevel) {
                break;  // åŒã˜ãƒ¬ãƒ™ãƒ«ã¾ãŸã¯ä¸Šä½ãƒ¬ãƒ™ãƒ«ã«åˆ°é”
            }
            childrenIndices.push(i);
        }
        
        return childrenIndices;
    }
    
    /**
     * Helper: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
     */
    private countTokens(text: string): number {
        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯tiktokenãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
        return Math.ceil(text.length / 4);
    }
}
```

### 2. Reasoning-Based Retrieval Implementation

#### 2.1 Tree Search Architecture

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
    .edgeLabel { color: #A6E22E !important; }
  '
}}%%

stateDiagram-v2
    [*] --> QueryAnalysis: User Query
    
    QueryAnalysis --> IntentExtraction: Parse Query
    IntentExtraction --> TreeTraversal: Extract Keywords & Intent
    
    TreeTraversal --> NodeEvaluation: Traverse Tree
    
    state NodeEvaluation {
        [*] --> CheckRelevance
        CheckRelevance --> ScoreNode: Calculate Relevance Score
        ScoreNode --> CheckChildren: Score > Threshold?
        CheckChildren --> [*]: Continue to Next Node
    }
    
    NodeEvaluation --> ReasoningChain: All Nodes Evaluated
    
    ReasoningChain --> RankNodes: Generate Reasoning
    RankNodes --> SelectTopK: Sort by Relevance
    
    SelectTopK --> ContextExtraction: Select Best Nodes
    ContextExtraction --> [*]: Return Node IDs & Text
    
    note right of ReasoningChain
        LLMãŒæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç”Ÿæˆ
        ãªãœã“ã‚Œã‚‰ã®ãƒãƒ¼ãƒ‰ãŒ
        é–¢é€£ã™ã‚‹ã‹ã‚’èª¬æ˜
    end note
```

#### 2.2 Tree Search Algorithm (TypeScript)

```typescript
// æ¨è«–ãƒ™ãƒ¼ã‚¹ãƒ„ãƒªãƒ¼æ¢ç´¢ã®å®Ÿè£…

interface SearchQuery {
    query: string;
    maxNodes?: number;
    expertKnowledge?: string;
}

interface SearchResult {
    thinking: string;
    nodeList: string[];
    confidence: number;
}

interface NodeWithScore {
    nodeId: string;
    title: string;
    summary?: string;
    relevanceScore: number;
    reasoning: string;
}

class ReasoningBasedTreeSearch {
    private llmClient: LLMClient;
    
    constructor(llmClient: LLMClient) {
        this.llmClient = llmClient;
    }
    
    /**
     * ãƒ¡ã‚¤ãƒ³æ¢ç´¢é–¢æ•°
     */
    async search(
        query: SearchQuery, 
        treeStructure: TreeNode[]
    ): Promise<SearchResult> {
        // Step 1: ã‚¯ã‚¨ãƒªåˆ†æ
        const queryAnalysis = await this.analyzeQuery(query);
        
        // Step 2: ãƒ„ãƒªãƒ¼æ¢ç´¢ã¨è©•ä¾¡
        const evaluatedNodes = await this.evaluateTreeNodes(
            queryAnalysis,
            treeStructure,
            query.expertKnowledge
        );
        
        // Step 3: ãƒãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        const rankedNodes = this.rankNodesByRelevance(evaluatedNodes);
        
        // Step 4: æœ€é©ãƒãƒ¼ãƒ‰é¸æŠ
        const selectedNodes = this.selectOptimalNodes(
            rankedNodes,
            query.maxNodes || 5
        );
        
        // Step 5: æ¨è«–ãƒã‚§ãƒ¼ãƒ³ç”Ÿæˆ
        const reasoningChain = this.generateReasoningChain(
            query.query,
            selectedNodes
        );
        
        return {
            thinking: reasoningChain,
            nodeList: selectedNodes.map(n => n.nodeId),
            confidence: this.calculateConfidence(selectedNodes)
        };
    }
    
    /**
     * Step 1: ã‚¯ã‚¨ãƒªåˆ†æ
     * LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’ç†è§£
     */
    private async analyzeQuery(query: SearchQuery): Promise<QueryAnalysis> {
        const prompt = `
        Analyze the following query and extract key information:
        Query: ${query.query}
        
        Extract:
        1. Main topic/subject
        2. Specific aspects or details requested
        3. Type of information needed (definition, example, comparison, etc.)
        4. Relevant keywords for search
        
        Format as JSON:
        {
            "mainTopic": "...",
            "specificAspects": [...],
            "informationType": "...",
            "keywords": [...]
        }
        `;
        
        const response = await this.llmClient.complete(prompt);
        return JSON.parse(response);
    }
    
    /**
     * Step 2: ãƒ„ãƒªãƒ¼ãƒãƒ¼ãƒ‰è©•ä¾¡
     * å„ãƒãƒ¼ãƒ‰ã®é–¢é€£æ€§ã‚’è©•ä¾¡
     */
    private async evaluateTreeNodes(
        queryAnalysis: QueryAnalysis,
        treeStructure: TreeNode[],
        expertKnowledge?: string
    ): Promise<NodeWithScore[]> {
        const evaluatedNodes: NodeWithScore[] = [];
        
        // å†å¸°çš„ã«ãƒ„ãƒªãƒ¼ã‚’æ¢ç´¢
        const evaluateNode = async (node: TreeNode, path: string[] = []) => {
            // ãƒãƒ¼ãƒ‰ã®é–¢é€£æ€§ã‚’è©•ä¾¡
            const relevance = await this.evaluateNodeRelevance(
                node,
                queryAnalysis,
                path,
                expertKnowledge
            );
            
            evaluatedNodes.push({
                nodeId: node.nodeId,
                title: node.title,
                summary: node.summary,
                relevanceScore: relevance.score,
                reasoning: relevance.reasoning
            });
            
            // å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«è©•ä¾¡
            for (const childNode of node.nodes) {
                await evaluateNode(childNode, [...path, node.title]);
            }
        };
        
        // ã™ã¹ã¦ã®ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã‹ã‚‰é–‹å§‹
        for (const rootNode of treeStructure) {
            await evaluateNode(rootNode);
        }
        
        return evaluatedNodes;
    }
    
    /**
     * å€‹åˆ¥ãƒãƒ¼ãƒ‰ã®é–¢é€£æ€§è©•ä¾¡
     */
    private async evaluateNodeRelevance(
        node: TreeNode,
        queryAnalysis: QueryAnalysis,
        path: string[],
        expertKnowledge?: string
    ): Promise<{score: number, reasoning: string}> {
        // è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        const prompt = `
        Evaluate the relevance of this document section to the query.
        
        Query Analysis:
        - Main Topic: ${queryAnalysis.mainTopic}
        - Specific Aspects: ${queryAnalysis.specificAspects.join(', ')}
        - Information Type: ${queryAnalysis.informationType}
        
        Document Section:
        - Title: ${node.title}
        - Path: ${path.join(' > ')}
        - Summary: ${node.summary || 'N/A'}
        ${expertKnowledge ? `\nExpert Knowledge: ${expertKnowledge}` : ''}
        
        Rate relevance from 0.0 to 1.0 and explain why.
        
        Format:
        {
            "score": 0.0-1.0,
            "reasoning": "explanation"
        }
        `;
        
        const response = await this.llmClient.complete(prompt);
        return JSON.parse(response);
    }
    
    /**
     * Step 3: ãƒãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
     * é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã§ãƒãƒ¼ãƒ‰ã‚’ã‚½ãƒ¼ãƒˆ
     */
    private rankNodesByRelevance(nodes: NodeWithScore[]): NodeWithScore[] {
        return nodes
            .filter(node => node.relevanceScore > 0.3)  // é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿
            .sort((a, b) => b.relevanceScore - a.relevanceScore);
    }
    
    /**
     * Step 4: æœ€é©ãƒãƒ¼ãƒ‰é¸æŠ
     * ã‚«ãƒãƒ¬ãƒƒã‚¸ã¨é‡è¤‡ã‚’è€ƒæ…®ã—ã¦æœ€é©ãªãƒãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚’é¸æŠ
     */
    private selectOptimalNodes(
        rankedNodes: NodeWithScore[],
        maxNodes: number
    ): NodeWithScore[] {
        const selected: NodeWithScore[] = [];
        const coveredTopics = new Set<string>();
        
        for (const node of rankedNodes) {
            if (selected.length >= maxNodes) break;
            
            // ãƒˆãƒ”ãƒƒã‚¯ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
            const nodeTopics = this.extractTopics(node.title);
            const isNewInformation = nodeTopics.some(
                topic => !coveredTopics.has(topic)
            );
            
            if (isNewInformation || node.relevanceScore > 0.8) {
                selected.push(node);
                nodeTopics.forEach(topic => coveredTopics.add(topic));
            }
        }
        
        return selected;
    }
    
    /**
     * Step 5: æ¨è«–ãƒã‚§ãƒ¼ãƒ³ç”Ÿæˆ
     * ãªãœã“ã‚Œã‚‰ã®ãƒãƒ¼ãƒ‰ãŒé¸ã°ã‚ŒãŸã‹ã‚’èª¬æ˜
     */
    private generateReasoningChain(
        query: string,
        selectedNodes: NodeWithScore[]
    ): string {
        let reasoning = `For the query "${query}", I identified ${selectedNodes.length} relevant sections:\n\n`;
        
        selectedNodes.forEach((node, index) => {
            reasoning += `${index + 1}. "${node.title}" (confidence: ${(node.relevanceScore * 100).toFixed(1)}%)\n`;
            reasoning += `   Reasoning: ${node.reasoning}\n\n`;
        });
        
        reasoning += "These sections were selected based on their direct relevance ";
        reasoning += "to the query topic and their potential to contain the requested information.";
        
        return reasoning;
    }
    
    /**
     * Helper: ä¿¡é ¼åº¦è¨ˆç®—
     */
    private calculateConfidence(nodes: NodeWithScore[]): number {
        if (nodes.length === 0) return 0;
        
        const avgScore = nodes.reduce((sum, node) => sum + node.relevanceScore, 0) / nodes.length;
        const topNodeScore = nodes[0]?.relevanceScore || 0;
        
        // å¹³å‡ã‚¹ã‚³ã‚¢ã¨æœ€é«˜ã‚¹ã‚³ã‚¢ã®åŠ é‡å¹³å‡
        return avgScore * 0.4 + topNodeScore * 0.6;
    }
    
    /**
     * Helper: ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
     */
    private extractTopics(title: string): string[] {
        // ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        return title.toLowerCase()
            .split(/\s+/)
            .filter(word => word.length > 3);
    }
}

// LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
interface LLMClient {
    complete(prompt: string): Promise<string>;
}

interface QueryAnalysis {
    mainTopic: string;
    specificAspects: string[];
    informationType: string;
    keywords: string[];
}
```

### 3. Advanced Features: Expert Knowledge Integration

#### 3.1 Expert Knowledge Integration Flow

```mermaid
%%{init: {
  'theme': 'dark',
  'themeCSS': '
    .nodeLabel { color: #FD971F !important; }
    .edgeLabel { color: #A6E22E !important; }
  '
}}%%

flowchart TB
    subgraph UserInput["ğŸ‘¤ User Input"]
        Query[Query]
        Context[Domain Context]
    end
    
    subgraph KnowledgeBase["ğŸ“š Expert Knowledge Base"]
        Rules[Domain Rules]
        Preferences[User Preferences]
        History[Search History]
    end
    
    subgraph KnowledgeRetrieval["ğŸ¯ Knowledge Selection"]
        Match[Pattern Matching]
        Relevance[Relevance Scoring]
        Select[Select Top Knowledge]
    end
    
    Query --> Match
    Context --> Match
    Rules --> Match
    Preferences --> Match
    History --> Match
    
    Match --> Relevance
    Relevance --> Select
    
    subgraph EnhancedSearch["ğŸ” Enhanced Tree Search"]
        BasePrompt[Base Search Prompt]
        KnowledgeInjection[Inject Expert Knowledge]
        EnhancedPrompt[Enhanced Prompt]
        
        Select --> KnowledgeInjection
        BasePrompt --> KnowledgeInjection
        KnowledgeInjection --> EnhancedPrompt
    end
    
    subgraph SearchExecution["âš¡ Search Execution"]
        TreeSearch[LLM Tree Search]
        Reasoning[Enhanced Reasoning]
        NodeSelection[Informed Node Selection]
        
        EnhancedPrompt --> TreeSearch
        TreeSearch --> Reasoning
        Reasoning --> NodeSelection
    end
    
    NodeSelection --> Results[Search Results]
    
    classDef inputClass fill:#272822,stroke:#FD971F,stroke-width:2px;
    classDef knowledgeClass fill:#272822,stroke:#66D9EF,stroke-width:2px;
    classDef searchClass fill:#272822,stroke:#A6E22E,stroke-width:2px;
    classDef outputClass fill:#272822,stroke:#F92672,stroke-width:2px;
```

#### 3.2 Expert Knowledge Implementation

```typescript
// å°‚é–€çŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

interface ExpertKnowledge {
    domain: string;
    rules: ExpertRule[];
    preferences: UserPreference[];
}

interface ExpertRule {
    condition: string;  // e.g., "query mentions EBITDA"
    guidance: string;   // e.g., "prioritize Item 7 (MD&A)"
    weight: number;     // 0.0 - 1.0
}

interface UserPreference {
    pattern: string;
    nodePreference: string[];
    reason: string;
}

class ExpertKnowledgeIntegrator {
    private knowledgeBase: Map<string, ExpertKnowledge>;
    
    /**
     * å°‚é–€çŸ¥è­˜ã‚’æ¤œç´¢ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
     */
    integrateKnowledge(
        basePrompt: string,
        query: string,
        domain: string
    ): string {
        const relevantKnowledge = this.retrieveRelevantKnowledge(query, domain);
        
        if (!relevantKnowledge || relevantKnowledge.length === 0) {
            return basePrompt;
        }
        
        // çŸ¥è­˜ã‚’æ§‹é€ åŒ–ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
        const knowledgeSection = `
Expert Knowledge and Preferences:
${relevantKnowledge.map(k => `- ${k.guidance}`).join('\n')}

Consider these domain-specific insights when selecting relevant nodes.
Prioritize sections that align with these expert guidelines.
        `;
        
        // ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çŸ¥è­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒ¿å…¥
        return basePrompt.replace(
            'Document tree structure:',
            `${knowledgeSection}\n\nDocument tree structure:`
        );
    }
    
    /**
     * ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹å°‚é–€çŸ¥è­˜ã‚’å–å¾—
     */
    private retrieveRelevantKnowledge(
        query: string,
        domain: string
    ): ExpertRule[] {
        const knowledge = this.knowledgeBase.get(domain);
        if (!knowledge) return [];
        
        const relevantRules: ExpertRule[] = [];
        
        for (const rule of knowledge.rules) {
            if (this.matchesCondition(query, rule.condition)) {
                relevantRules.push(rule);
            }
        }
        
        // é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        return relevantRules.sort((a, b) => b.weight - a.weight);
    }
    
    /**
     * æ¡ä»¶ãƒãƒƒãƒãƒ³ã‚°
     */
    private matchesCondition(query: string, condition: string): boolean {
        // ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        const conditionKeywords = condition.toLowerCase().split(' ');
        const queryLower = query.toLowerCase();
        
        return conditionKeywords.some(keyword => 
            queryLower.includes(keyword)
        );
    }
}
```

### 4. Performance Optimizations

#### 4.1 Caching Strategy

```typescript
// ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ã®å®Ÿè£…

class TreeSearchCache {
    private cache: Map<string, CachedResult>;
    private maxCacheSize = 1000;
    private ttl = 3600000; // 1 hour
    
    interface CachedResult {
        result: SearchResult;
        timestamp: number;
        hitCount: number;
    }
    
    /**
     * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®å–å¾—
     */
    get(queryHash: string): SearchResult | null {
        const cached = this.cache.get(queryHash);
        
        if (!cached) return null;
        
        // TTLãƒã‚§ãƒƒã‚¯
        if (Date.now() - cached.timestamp > this.ttl) {
            this.cache.delete(queryHash);
            return null;
        }
        
        // ãƒ’ãƒƒãƒˆã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
        cached.hitCount++;
        return cached.result;
    }
    
    /**
     * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸ã®ä¿å­˜
     */
    set(queryHash: string, result: SearchResult): void {
        // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        if (this.cache.size >= this.maxCacheSize) {
            this.evictLRU();
        }
        
        this.cache.set(queryHash, {
            result,
            timestamp: Date.now(),
            hitCount: 0
        });
    }
    
    /**
     * LRUå‰Šé™¤
     */
    private evictLRU(): void {
        let minHits = Infinity;
        let evictKey = '';
        
        for (const [key, value] of this.cache.entries()) {
            if (value.hitCount < minHits) {
                minHits = value.hitCount;
                evictKey = key;
            }
        }
        
        if (evictKey) {
            this.cache.delete(evictKey);
        }
    }
}
```

## ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¦ç´„

### æ ¸å¿ƒã‚³ãƒ³ã‚»ãƒ—ãƒˆ
**ã€Œæ–‡æ›¸ã®ç‰©ç†çš„æ§‹é€ ã‚’è«–ç†çš„æ¨è«–ã§æ¢ç´¢ã™ã‚‹ã€**

å¾“æ¥ã®RAGãŒã€Œæ„å‘³çš„ã«ä¼¼ã¦ã„ã‚‹éƒ¨åˆ†ã‚’æ¢ã™ã€ã®ã«å¯¾ã—ã€PageIndexã¯ã€Œæ–‡æ›¸ã®æ§‹é€ ã‚’ç†è§£ã—ã¦ã€è«–ç†çš„ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’æ¨è«–ã§è¦‹ã¤ã‘ã‚‹ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚

### 2æ®µéšã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
æ–‡æ›¸ â†’ [æ§‹é€ åŒ–] â†’ ãƒ„ãƒªãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ â†’ [æ¨è«–æ¢ç´¢] â†’ é–¢é€£ã‚»ã‚¯ã‚·ãƒ§ãƒ³
```

#### ç¬¬1æ®µéšï¼šæ§‹é€ ã®æ˜ç¤ºåŒ–
- **äººé–“ã®èª­ã¿æ–¹ã‚’æ¨¡å€£**: ç›®æ¬¡ã‚„è¦‹å‡ºã—ã‹ã‚‰æ–‡æ›¸ã®éšå±¤æ§‹é€ ã‚’æŠ½å‡º
- **è‡ªç„¶ãªå¢ƒç•Œã‚’ä¿æŒ**: äººå·¥çš„ãªãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã§ã¯ãªãã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è‡ªç„¶ãªåŒºåˆ‡ã‚Šã‚’ç¶­æŒ
- **ã‚¹ã‚¿ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹éšå±¤æ§‹ç¯‰**: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¬ãƒ™ãƒ«ã‹ã‚‰è¦ªå­é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰

#### ç¬¬2æ®µéšï¼šæ¨è«–ã«ã‚ˆã‚‹æ¢ç´¢
- **æ„å›³ç†è§£**: ã‚¯ã‚¨ãƒªã‹ã‚‰ã€Œä½•ã‚’çŸ¥ã‚ŠãŸã„ã‹ã€ã‚’åˆ†æ
- **æ§‹é€ èªè­˜å‹æ¢ç´¢**: ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’ç†è§£ã—ãªãŒã‚‰é–¢é€£ãƒãƒ¼ãƒ‰ã‚’è©•ä¾¡
- **èª¬æ˜å¯èƒ½ãªé¸æŠ**: ãªãœãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒé¸ã°ã‚ŒãŸã‹ã‚’æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã§èª¬æ˜

### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é©æ–°æ€§

1. **æ§‹é€ ä¿å­˜å‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**
   - æ–‡æ›¸ã®è«–ç†æ§‹é€ ã‚’å®Œå…¨ã«ä¿æŒ
   - ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®é–¢ä¿‚æ€§ã‚’ç¶­æŒ
   - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–­çµ¶ã‚’é˜²ã

2. **æ¨è«–é§†å‹•å‹æ¤œç´¢**
   - é¡ä¼¼æ€§ã§ã¯ãªãé–¢é€£æ€§ã‚’åˆ¤æ–­
   - LLMã®æ¨è«–èƒ½åŠ›ã‚’æ´»ç”¨ã—ãŸæ„å‘³ç†è§£
   - å°‚é–€çŸ¥è­˜ã®ç›´æ¥çµ±åˆãŒå¯èƒ½

3. **é©å¿œçš„æœ€é©åŒ–**
   - ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ãå‹•çš„ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸
   - é‡è¤‡å›é¿ã¨ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€å¤§åŒ–
   - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–

### ä¸€è¨€ã§è¡¨ç¾ã™ã‚‹ã¨

> **PageIndexã¯ã€Œæ–‡æ›¸ã‚’èª­ã‚€äººé–“ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã€ã‚’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŒ–ã—ãŸã‚·ã‚¹ãƒ†ãƒ **

äººé–“ãŒå°‚é–€æ›¸ã‚’èª­ã‚€ã¨ãã€ã¾ãšç›®æ¬¡ã‚’è¦‹ã¦æ§‹é€ ã‚’æŠŠæ¡ã—ã€å¿…è¦ãªæƒ…å ±ãŒã‚ã‚Šãã†ãªç« ã‚’æ¨è«–ã§ç‰¹å®šã™ã‚‹ã€‚PageIndexã¯ã¾ã•ã«ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’å†ç¾ã—ã¦ã„ã¾ã™ã€‚

## Q&Aï¼šå‹•ä½œåŸç†ã®è©³ç´°

### Q: Sectionã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€Œé–¢ä¿‚ã‚ã‚Šãã†ã€ã¨ã„ã†ã‚‚ã®ã‚’å–ã‚Šå‡ºã—ã¦ã€ãã®Sectionå†…ã«ã‚ã‚‹æ–‡ç« ã‹ã‚‰å–ã£ã¦ãã‚‹ã¨ã„ã†ã“ã¨ï¼Ÿ

### A: éƒ¨åˆ†çš„ã«æ­£ã—ã„ãŒã€ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹

**å˜ã«ã‚¿ã‚¤ãƒˆãƒ«ã ã‘ã§ãªãã€è¤‡æ•°ã®è¦ç´ ã‚’ç·åˆçš„ã«åˆ¤æ–­**ã—ã¾ã™ï¼š

#### å®Ÿéš›ã®è©•ä¾¡è¦ç´ 

```typescript
{
  "title": "Financial Stability",           // ã‚¿ã‚¤ãƒˆãƒ«
  "summary": "Federal Reserve monitors...",  // ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦ç´„
  "path": ["Annual Report", "Part II"],     // æ–‡æ›¸å†…ã®ä½ç½®
  "node_id": "0006",
  "child_nodes": ["Monitoring", "Cooperation"] // å­ã‚»ã‚¯ã‚·ãƒ§ãƒ³
}
```

#### å…·ä½“ä¾‹ï¼šã€Œ2023å¹´ã®EBITDAèª¿æ•´é …ç›®ã«ã¤ã„ã¦ã€ã‚’æ¤œç´¢ã™ã‚‹å ´åˆ

**Step 1: ãƒ„ãƒªãƒ¼æ¢ç´¢**
```
ğŸ“„ Annual Report 2023
â”œâ”€â”€ ğŸ“‚ Financial Overview
â”‚   â”œâ”€â”€ ğŸ“„ Income Statement
â”‚   â”œâ”€â”€ ğŸ“„ Balance Sheet
â”‚   â””â”€â”€ ğŸ“„ Cash Flow
â”œâ”€â”€ ğŸ“‚ Management Discussion (MD&A)  â† âœ… é–¢é€£æ€§é«˜
â”‚   â”œâ”€â”€ ğŸ“„ Financial Results
â”‚   â”œâ”€â”€ ğŸ“„ Non-GAAP Measures       â† âœ… é–¢é€£æ€§é«˜
â”‚   â””â”€â”€ ğŸ“„ Adjustments and Reconciliations â† âœ… é–¢é€£æ€§é«˜
â””â”€â”€ ğŸ“‚ Notes to Financial Statements
    â””â”€â”€ ğŸ“„ Note 15: Segment Reporting
```

**Step 2: æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹**
```json
{
  "thinking": "EBITDAã¯éGAAPæŒ‡æ¨™ãªã®ã§ã€MD&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®
              'Non-GAAP Measures'ã¨'Adjustments'ã«è©³ç´°ãŒã‚ã‚‹ã¯ãšã€‚
              Financial Overviewã¯æ¦‚è¦ã®ã¿ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚",
  "selected_nodes": ["0007", "0008", "0009"],
  "confidence": 0.92
}
```

**Step 3: ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**
é¸ã°ã‚ŒãŸãƒãƒ¼ãƒ‰ã‹ã‚‰**å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡**ã‚’å–å¾—

#### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

1. **ã‚¿ã‚¤ãƒˆãƒ«ã¯å…¥å£ã«éããªã„**
   - ã‚¿ã‚¤ãƒˆãƒ« + è¦ç´„ + éšå±¤æ§‹é€  + æ–‡è„ˆ ã‚’ç·åˆåˆ¤æ–­
   - ã€Œè²¡å‹™è«¸è¡¨ã€ã‚ˆã‚Šã€ŒçµŒå–¶è€…ã«ã‚ˆã‚‹è­°è«–ã€ã®æ–¹ãŒEBITDAè©³ç´°ãŒã‚ã‚‹ã€ã¨ã„ã†æ¨è«–

2. **éšå±¤æ§‹é€ ã®æ´»ç”¨**
   ```
   MD&A > Non-GAAP Measures > EBITDA Reconciliation
   ```
   ã“ã®éšå±¤ãƒ‘ã‚¹ã‹ã‚‰ã€Œã“ã“ã«ç­”ãˆãŒã‚ã‚‹ã€ã¨æ¨è«–

3. **è¦ç´„ã®é‡è¦æ€§**
   å„ãƒãƒ¼ãƒ‰ã«ã¯è¦ç´„ãŒã‚ã‚Šã€ã‚¿ã‚¤ãƒˆãƒ«ã ã‘ã§ã¯åˆ†ã‹ã‚‰ãªã„å†…å®¹ã‚’æŠŠæ¡
   ```json
   {
     "title": "Note 15",  // ä¸€è¦‹é–¢ä¿‚ãªã•ãã†
     "summary": "Includes EBITDA breakdown by segment..."  // å®Ÿã¯é–¢é€£
   }
   ```

#### ãªãœã“ã‚ŒãŒå¼·åŠ›ã‹

**å¾“æ¥ã®ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢**:
```
ã€ŒEBITDAèª¿æ•´ã€ã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯ã‚’æ©Ÿæ¢°çš„ã«æ¤œç´¢
â†’ æ–‡è„ˆãŒãƒãƒ©ãƒãƒ©ãªæ–­ç‰‡ãŒè¿”ã•ã‚Œã‚‹
```

**PageIndex**:
```
ã€ŒEBITDAèª¿æ•´ã®èª¬æ˜ãŒã‚ã‚Šãã†ãªç« ã€ã‚’æ¨è«–ã§ç‰¹å®š
â†’ å®Œå…¨ãªæ–‡è„ˆã‚’æŒã¤ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ãŒè¿”ã•ã‚Œã‚‹
```

### ã¾ã¨ã‚

PageIndexã¯ï¼š
1. âœ… ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é–¢ä¿‚æ€§ã‚’åˆ¤æ–­
2. âœ… **ã•ã‚‰ã«**è¦ç´„ãƒ»éšå±¤ãƒ»æ–‡è„ˆã‚‚è€ƒæ…®
3. âœ… **æ¨è«–ã§**æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
4. âœ… é¸ã°ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®**å®Œå…¨ãªãƒ†ã‚­ã‚¹ãƒˆ**ã‚’å–å¾—

ã¤ã¾ã‚Šã€**ã€Œäººé–“ãŒç›®æ¬¡ã¨æ¦‚è¦ã‚’è¦‹ã¦ã€ã©ã®ç« ã‚’èª­ã‚€ã¹ãã‹åˆ¤æ–­ã™ã‚‹ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚’å†ç¾**ã—ã¦ã„ã‚‹ã®ã§ã™ã€‚

### Q: ã‚¯ã‚¨ãƒªã®å†…å®¹ãŒæ§˜ã€…ãªSectionã«æ•£ã‚‰ã°ã£ã¦ã„ã‚‹å ´åˆã¯ã©ã†ãªã‚‹ã®ï¼Ÿ

### A: PageIndexã¯æ•£åœ¨æƒ…å ±ã‚’åŒ…æ‹¬çš„ã«åé›†ã™ã‚‹é«˜åº¦ãªä»•çµ„ã¿ã‚’æŒã¤

#### è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¾ãŸãŒã‚‹æƒ…å ±ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    Query["è¤‡é›‘ãªã‚¯ã‚¨ãƒª<br/>ä¾‹: 2023å¹´ã®æ¥­ç¸¾ã¨ä»Šå¾Œã®æˆ¦ç•¥"]
    
    Query --> Analysis[ã‚¯ã‚¨ãƒªåˆ†è§£]
    Analysis --> Aspect1["æ¥­ç¸¾ãƒ‡ãƒ¼ã‚¿"]
    Analysis --> Aspect2["æˆ¦ç•¥è¨ˆç”»"]
    Analysis --> Aspect3["æ™‚ç³»åˆ—æƒ…å ±"]
    
    Aspect1 --> Node1["Financial Results<br/>node: 0005"]
    Aspect1 --> Node2["Segment Performance<br/>node: 0012"]
    
    Aspect2 --> Node3["Strategic Initiatives<br/>node: 0018"]
    Aspect2 --> Node4["Future Outlook<br/>node: 0021"]
    
    Aspect3 --> Node5["Q4 2023 Summary<br/>node: 0008"]
    
    Node1 --> Aggregate[è¤‡æ•°ãƒãƒ¼ãƒ‰é¸æŠ]
    Node2 --> Aggregate
    Node3 --> Aggregate
    Node4 --> Aggregate
    Node5 --> Aggregate
    
    Aggregate --> Result["çµ±åˆã•ã‚ŒãŸ<br/>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"]
```

#### å…·ä½“ä¾‹ï¼šã€Œ2023å¹´ã®ESGå–ã‚Šçµ„ã¿ã¨ãã®è²¡å‹™ã¸ã®å½±éŸ¿ã€

ã“ã®ã‚¯ã‚¨ãƒªã¯è¤‡æ•°ã®å´é¢ã‚’å«ã‚€ï¼š
- ESGæ´»å‹•ã®å†…å®¹
- è²¡å‹™ã¸ã®å½±éŸ¿
- 2023å¹´ã®æ™‚ç³»åˆ—

**Step 1: ã‚¯ã‚¨ãƒªåˆ†æã¨è¤‡æ•°å´é¢ã®èªè­˜**

```json
{
  "query_analysis": {
    "main_topics": ["ESG", "è²¡å‹™å½±éŸ¿"],
    "time_frame": "2023",
    "information_types": ["æ´»å‹•å†…å®¹", "å®šé‡çš„å½±éŸ¿", "å› æœé–¢ä¿‚"],
    "expected_locations": [
      "Sustainability Report section",
      "Financial Impact section",
      "MD&A section",
      "Risk Factors section"
    ]
  }
}
```

**Step 2: è¤‡æ•°ãƒãƒ¼ãƒ‰ã®ä¸¦åˆ—è©•ä¾¡**

```typescript
// PageIndexã®å®Ÿéš›ã®å‹•ä½œ
class MultiSectionRetrieval {
    async retrieveForComplexQuery(query: string, tree: TreeNode[]) {
        // 1. ã‚¯ã‚¨ãƒªã®è¤‡æ•°å´é¢ã‚’è­˜åˆ¥
        const aspects = await this.identifyQueryAspects(query);
        
        // 2. å„å´é¢ã«å¯¾ã—ã¦é–¢é€£ãƒãƒ¼ãƒ‰ã‚’æ¢ç´¢
        const relevantNodes: Map<string, NodeWithScore[]> = new Map();
        
        for (const aspect of aspects) {
            const nodes = await this.findNodesForAspect(aspect, tree);
            relevantNodes.set(aspect.name, nodes);
        }
        
        // 3. ãƒãƒ¼ãƒ‰é–“ã®é–¢é€£æ€§ã‚’åˆ†æ
        const nodeRelationships = this.analyzeNodeRelationships(relevantNodes);
        
        // 4. æœ€é©ãªãƒãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚’é¸æŠ
        return this.selectOptimalNodeSet(relevantNodes, nodeRelationships);
    }
}
```

**Step 3: å®Ÿéš›ã®æ¤œç´¢çµæœ**

```json
{
  "selected_nodes": [
    {
      "node_id": "0015",
      "title": "Environmental Initiatives",
      "reason": "ESGæ´»å‹•ã®è©³ç´°",
      "relevance_score": 0.95
    },
    {
      "node_id": "0023",
      "title": "Financial Performance - Sustainability Impact",
      "reason": "è²¡å‹™ã¸ã®ç›´æ¥çš„å½±éŸ¿",
      "relevance_score": 0.92
    },
    {
      "node_id": "0031",
      "title": "Cost Savings from Green Operations",
      "reason": "å®šé‡çš„ãªè²¡å‹™åŠ¹æœ",
      "relevance_score": 0.88
    },
    {
      "node_id": "0042",
      "title": "Risk Management - Climate Change",
      "reason": "ESGãƒªã‚¹ã‚¯ã®è²¡å‹™å½±éŸ¿",
      "relevance_score": 0.85
    }
  ],
  "thinking": "ESGæƒ…å ±ã¯è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†æ•£ã€‚ç’°å¢ƒã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æ´»å‹•è©³ç´°ã€
              è²¡å‹™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å½±éŸ¿æ•°å€¤ã€ãƒªã‚¹ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å°†æ¥å½±éŸ¿ã‚’ç™ºè¦‹ã€‚
              ã“ã‚Œã‚‰ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§åŒ…æ‹¬çš„ãªå›ç­”ãŒå¯èƒ½ã€‚"
}
```

#### PageIndexã®å¼·ã¿ï¼šæ•£åœ¨æƒ…å ±ã®å‡¦ç†

**1. ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **

```typescript
// é‡è¤‡ã‚’é¿ã‘ãªãŒã‚‰æƒ…å ±ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æœ€å¤§åŒ–
selectOptimalNodes(candidates: NodeWithScore[], maxNodes: number) {
    const selected = [];
    const coveredTopics = new Set();
    const coveredAspects = new Set();
    
    // ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    candidates.sort((a, b) => b.score - a.score);
    
    for (const node of candidates) {
        // æ–°ã—ã„æƒ…å ±ã‚’æä¾›ã™ã‚‹ã‹ç¢ºèª
        const newInfo = this.providesNewInformation(
            node, 
            coveredTopics, 
            coveredAspects
        );
        
        if (newInfo || node.score > 0.9) {
            selected.push(node);
            this.updateCoverage(node, coveredTopics, coveredAspects);
        }
        
        if (selected.length >= maxNodes) break;
    }
    
    return selected;
}
```

**2. ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹èªè­˜**

```
è²¡å‹™å ±å‘Šæ›¸ã®æ§‹é€ ä¾‹ï¼š
â”œâ”€â”€ Executive Summary
â”‚   â””â”€â”€ "è©³ç´°ã¯Section 3.2å‚ç…§" â†’ ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹æ¤œå‡º
â”œâ”€â”€ Section 3.2: ESG Impact
â”‚   â””â”€â”€ "è²¡å‹™æ•°å€¤ã¯Appendix A" â†’ è¿½åŠ ãƒãƒ¼ãƒ‰å¿…è¦
â””â”€â”€ Appendix A: Financial Tables
    â””â”€â”€ å®Ÿéš›ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿
```

**3. æƒ…å ±ã®è£œå®Œæ€§è©•ä¾¡**

```json
{
  "node_relationships": {
    "complementary": [
      ["0015", "0023"],  // ESGæ´»å‹•ã¨è²¡å‹™å½±éŸ¿ã¯è£œå®Œé–¢ä¿‚
      ["0031", "0042"]   // ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨ãƒªã‚¹ã‚¯ç®¡ç†ã¯è£œå®Œé–¢ä¿‚
    ],
    "redundant": [
      ["0016", "0017"]   // é‡è¤‡ã™ã‚‹æƒ…å ±
    ],
    "hierarchical": [
      ["0015", "0018", "0019"]  // è¦ªå­é–¢ä¿‚
    ]
  }
}
```

#### å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«RAGã¨ã®é•ã„

**ãƒ™ã‚¯ãƒˆãƒ«RAGï¼ˆæ•£åœ¨æƒ…å ±ã§è‹¦æˆ¦ï¼‰**:
```
å•é¡Œç‚¹ï¼š
- å„ãƒãƒ£ãƒ³ã‚¯ãŒç‹¬ç«‹ã—ã¦è©•ä¾¡ã•ã‚Œã‚‹
- ãƒãƒ£ãƒ³ã‚¯é–“ã®é–¢ä¿‚æ€§ãŒå¤±ã‚ã‚Œã‚‹
- åŒã˜ãƒˆãƒ”ãƒƒã‚¯ã®ç•°ãªã‚‹å´é¢ãŒè¦‹é€ƒã•ã‚Œã‚‹
- Top-Kã§é‡è¦ãªè£œå®Œæƒ…å ±ãŒæ¼ã‚Œã‚‹
```

**PageIndexï¼ˆæ•£åœ¨æƒ…å ±ã«å¼·ã„ï¼‰**:
```
åˆ©ç‚¹ï¼š
âœ… æ–‡æ›¸å…¨ä½“ã®æ§‹é€ ã‚’ç†è§£
âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®é–¢ä¿‚ã‚’ä¿æŒ
âœ… è¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰åŒ…æ‹¬çš„ã«åé›†
âœ… ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’è¿½è·¡
âœ… æƒ…å ±ã®è£œå®Œæ€§ã‚’è©•ä¾¡
```

#### å®Ÿéš›ã®æ€§èƒ½å·®ã®ä¾‹

```
ã‚±ãƒ¼ã‚¹: "2023å¹´ã®M&Aæ´»å‹•ã¨ãã®çµ±åˆã‚³ã‚¹ãƒˆã€ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœ"

ãƒ™ã‚¯ãƒˆãƒ«RAG:
- M&Aã®ç™ºè¡¨æ–‡ã®ã¿å–å¾—ï¼ˆé¡ä¼¼åº¦é«˜ï¼‰
- çµ±åˆã‚³ã‚¹ãƒˆã¯åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆè¦‹é€ƒã—ï¼‰
- ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœã¯å°†æ¥äºˆæ¸¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆè¦‹é€ƒã—ï¼‰
â†’ éƒ¨åˆ†çš„ãªå›ç­”

PageIndex:
- M&Aæ¦‚è¦ï¼ˆCorporate Actionsï¼‰
- çµ±åˆã‚³ã‚¹ãƒˆè©³ç´°ï¼ˆFinancial Notesï¼‰  
- ã‚·ãƒŠã‚¸ãƒ¼äºˆæ¸¬ï¼ˆManagement Outlookï¼‰
- é–¢é€£ãƒªã‚¹ã‚¯ï¼ˆRisk Factorsï¼‰
â†’ åŒ…æ‹¬çš„ãªå›ç­”
```

#### ã¾ã¨ã‚ï¼šæ•£åœ¨æƒ…å ±ã¸ã®å¯¾å¿œ

PageIndexã¯æ•£åœ¨æƒ…å ±ã«å¯¾ã—ã¦ï¼š

1. **ãƒãƒ«ãƒã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆ†æ**: ã‚¯ã‚¨ãƒªã‚’è¤‡æ•°ã®å´é¢ã«åˆ†è§£
2. **ä¸¦åˆ—ãƒãƒ¼ãƒ‰æ¢ç´¢**: å„å´é¢ã«å¯¾ã—ã¦é–¢é€£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
3. **ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€é©åŒ–**: é‡è¤‡ã‚’é¿ã‘ã¤ã¤æƒ…å ±ã‚’ç¶²ç¾…
4. **é–¢ä¿‚æ€§ç†è§£**: ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®å‚ç…§é–¢ä¿‚ã‚’èªè­˜
5. **çµ±åˆçš„å–å¾—**: è£œå®Œçš„ãªæƒ…å ±ã‚’å«ã‚€è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ

ã“ã‚Œã«ã‚ˆã‚Šã€**ã€Œäººé–“ãŒç´¢å¼•ã‚’ä½¿ã£ã¦è¤‡æ•°ã®ç« ã‚’å‚ç…§ã—ãªãŒã‚‰ç·åˆçš„ã«ç†è§£ã™ã‚‹ã€ãƒ—ãƒ­ã‚»ã‚¹**ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

## Resources & References

- GitHub Repository: https://github.com/VectifyAI/PageIndex
- Documentation: https://docs.pageindex.ai
- Dashboard: https://dash.pageindex.ai
- FinanceBench Results: https://github.com/VectifyAI/Mafin2.5-FinanceBench
- Research Blog: https://vectify.ai/blog/Mafin2.5